<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>HN - bigdata</title><link href="http://localhost:8000/" rel="alternate"></link><link href="http://localhost:8000/feeds/bigdata.atom.xml" rel="self"></link><id>http://localhost:8000/</id><updated>2019-06-02T00:00:00+05:30</updated><entry><title>Bigdata Session 1 - Hadoop with docker</title><link href="http://localhost:8000/bigdata-session-1-hadoop-with-docker.html" rel="alternate"></link><published>2019-06-02T00:00:00+05:30</published><updated>2019-06-02T00:00:00+05:30</updated><author><name>HN</name></author><id>tag:localhost,2019-06-02:/bigdata-session-1-hadoop-with-docker.html</id><summary type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This blog series includes 4 worshop session I conducted at Engieneering colleges
and in my office.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Hadoop with docker is mainly usable for easy prototype and learning purpose, also
it helps to setup quickly setup hadoop cluster on your laptop, or on multiple
machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop with …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This blog series includes 4 worshop session I conducted at Engieneering colleges
and in my office.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Hadoop with docker is mainly usable for easy prototype and learning purpose, also
it helps to setup quickly setup hadoop cluster on your laptop, or on multiple
machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop with docker &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_hadoop_cluster"&gt;1. Hadoop Cluster&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Main motive of the session is how we can easily test the hadoop
cluster and related tools locally or on a small cluster. We will setup the hadoop
in cluster mode. Hadoop cluster mode means, the individual components are runs
separately on single machine on separate JVMs or running on multiple machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Here we will use the docker based setup to quickly play with main
features of hadoop.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Main Hadoop services are:-&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;namenode ( storage )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;datanode ( storage )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;resource manager ( computation )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;node manager ( computation )&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;One hadoop cluster can formed by one namenode and multiple datanodes.
The resource manager runs on same machine as namenode ( simpler setup).
Responsibility of namenode is store metadata about distributed
filesystem (hdfs). Datanode actually stores the data in blob form,
and nodemanager will be running on each datanode to handle actual
computation requests from resource manager.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;2. Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As all cluster environments, network address resolution for each node is a key requirement
for stable setup. Ideally a local DNS setup which permanently allocate a hostnames
to all the nodes in the network. Or we can manually set the hostnames without DNS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
This material is verified on Mac and Ubuntu.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_jdk_1_8"&gt;2.1. JDK 1.8&lt;/h3&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Not required if you try Docker based setup.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Ensure you have jdk 1.8+ available on your machine, oracle jdk is
recommended.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_install_docker"&gt;2.2. Install Docker&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Ensue you have latest version of docker is setup on your laptop.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Docker Version: &lt;code&gt;18.x.x&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_for_ubuntu_debian_machines"&gt;For Ubuntu / Debian machines&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Follow this link and install the docker with correct given bellow,&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" class="bare"&gt;https://docs.docker.com/install/linux/docker-ce/ubuntu/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_for_mac"&gt;For Mac&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/docker-for-mac/install/" class="bare"&gt;https://docs.docker.com/docker-for-mac/install/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For other distros, please help yourself ;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_get_hadoop_docker_image"&gt;2.3. Get hadoop docker image&lt;/h3&gt;

&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_from_docker_hub"&gt;2.4. From docker hub&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker pull haridasn/hadoop-2.8.5:latest&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_build_the_docker_image_locally_optional"&gt;2.5. Build the docker image locally (Optional)&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;git clone https://github.com/haridas/hadoop-env
cd hadoop-env/docker
docker build -t hadoop-2.8.5:latest&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_set_correct_hostnames_for_multi_node_cluster_setup_optional"&gt;2.6. Set correct hostnames for multi-node cluster setup (Optional)&lt;/h3&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you are setting the cluster on same machine with docker, you can skip this step.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Each node in the host machine can reach each other using the hostname.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Hadoop cluster setup using multiple physical machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_set_hostnames_correctly"&gt;set hostnames correctly.&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;All the nodes place the same set of values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;cat /etc/hosts
master  &amp;lt;ip-address&amp;gt;
node1
node2
node3
..
..&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_set_the_hostname_of_the_machine_to_match_this_address"&gt;Set the hostname of the machine to match this address.&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;edit &lt;code&gt;/etc/hostname&lt;/code&gt;
edit &lt;code&gt;/etc/hosts&lt;/code&gt; and replace any occurrence of old hostname with new one.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_update_and_check_hostname_changed_correctly"&gt;Update and check hostname changed correctly&lt;/h5&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;sudo hostname &amp;lt;hostname&amp;gt;
hostname&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Cross check all the machines have correct set of hostnames before going to next
step.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_setup_cluster_on_single_machine_using_docker"&gt;3. Setup cluster on single machine using docker&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We are using the docker container mainly for process isolation,
for a simpler
setup on single machine we make use of the same network stack as the host machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_a_docker_network"&gt;3.1. Create a docker network&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For clean hostname resoluation under docker environment, we have
to create a docker network; which will internally provide a DNS
resoluation on the virtual network where all the containers reside.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker network create hadoop-nw&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We will use this network to launch all our container, which will
internally allocate all the containers into this network. So we will
get the hostname resoluation by default. For the non-docker deployment we have
to setup all these externally.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_start_namenode_and_resource_manager"&gt;3.2. Start namenode and resource Manager&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;docker run -it -d --name namenode --network hadoop-nw haridasn/hadoop-2.8.5:latest namenode

# check container is running
docker ps -a

# Check container logs
docker logs -f namenode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To get the &lt;code&gt;namenode&lt;/code&gt; ip, attach to the namenode docker container,
We need this for starting the datanodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker exec -it namenode bash
ifconfig&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_start_datanode_and_resource_manager"&gt;3.3. Start datanode and resource manager&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker run -it -d --name datanode1 \
    --network hadoop-nw haridasn/hadoop-2.8.5:latest datanode &amp;lt;name-node-ip&amp;gt;

docker ps -a

docker logs -f datanode1

.
# If you want launch more datanodes.

docker run -it -d --name datanode2 \
    --network hadoop-nw haridasn/hadoop-2.8.5:latest datanode &amp;lt;name-node-ip&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_get_the_client_tools_setup_on_another_docker"&gt;3.4. Get the client tools setup on another docker&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;code&gt;yarn&lt;/code&gt;, &lt;code&gt;hdfs&lt;/code&gt; clinet commands used to submit jobs and see the hdfs
files respectively are loaded in another docker. Lets use that as our workbench
to play with our hadoop cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Start the docker container to test our cluster.
docker run -it --rm --name hadoop-cli --network hadoop-nw haridasn/hadoop-cli:latest

# Get the configuration from running nodes.
docker cp namenode:/opt/hadoop/etc etc
docker cp etc hadoop-cli:/opt/hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_check_hdfs"&gt;3.5. Check hdfs&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;./bin/hdfs dfs -ls /

# copy files into hdfs
./bin/hdfs dfs -put /var/log/supervisor /logs
./bin/hdfs dfs -put /etc/passwd /passwd

# Copy files inside hdfs
./bin/hdfs dfs -cp /passwd /passwdr&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_check_resource_manager_works_fine"&gt;3.6. Check Resource manager works fine&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;./bin/yarn jar `pwd`/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar
pi 1 1

./bin/yarn jar `pwd`/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar
wordcount /logs/ /out/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_other_bigdata_tools_on_hadoop_environment"&gt;4. Other Bigdata tools on hadoop environment&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_pig"&gt;4.1. Pig&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A simpler command oriented interface to do the map-reduce jobs over
hadoop cluster. You can think this as a bash scripting over hdfs
and yarn map-reduce to quickly analyse data on hdfs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_download_and_extract_it"&gt;Download and extract it&lt;/h5&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;wget http://mirrors.estointernet.in/apache/pig/pig-0.17.0/pig-0.17.0.tar.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_setup_pig_and_configure_it_with_hadoop_cluster"&gt;Setup pig and configure it with hadoop cluster.&lt;/h5&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;export PIG_HOME=&amp;lt;path-to-pig-home&amp;gt;
export PATH=$PATH:$PIG_HOME/bin
export PIG_CLASSPATH=&amp;lt;path-to-hadoop-conf-dir&amp;gt;

pig&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_load_some_data_into_hdfs"&gt;Load some data into hdfs&lt;/h5&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;./bin/hdfs dfs -mkdir /pig
./bin/hdfs dfs -put pig/tutorial/data /pig/data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_pig_commandline_tool"&gt;Pig commandline tool&lt;/h5&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;$ pig

raw = LOAD '/pig/data/excite-small.log' USING PigStorage('\t') AS (user, time,query);

user = filter raw by $2=='powwow.com';

dump user&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_hive"&gt;4.2. Hive&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;SQL interface over hadoop system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="http://mirrors.estointernet.in/apache/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz" class="bare"&gt;http://mirrors.estointernet.in/apache/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_presentation"&gt;5. Presentation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/HKFon6Yn1cmXqa" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/haridasnss/bigdata-and-hadoop-with-docker" title="Bigdata and Hadoop with Docker" target="_blank"&gt;Bigdata and Hadoop with Docker&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="https://www.slideshare.net/haridasnss" target="_blank"&gt;haridasnss&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="hadoop"></category><category term="yarn"></category><category term="hdfs"></category></entry><entry><title>Bigdata Session 2 - Apache Spark</title><link href="http://localhost:8000/bigdata-session-2-apache-spark.html" rel="alternate"></link><published>2019-06-02T00:00:00+05:30</published><updated>2019-06-02T00:00:00+05:30</updated><author><name>HN</name></author><id>tag:localhost,2019-06-02:/bigdata-session-2-apache-spark.html</id><summary type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This blog series includes 4 worshop session I conducted at Engieneering colleges
and in my office.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This workshop we cover how spark internals works and how we can configure spark
with apache yarn resource manager. I&amp;#8217;m using docker based setup for this
workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This blog series includes 4 worshop session I conducted at Engieneering colleges
and in my office.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This workshop we cover how spark internals works and how we can configure spark
with apache yarn resource manager. I&amp;#8217;m using docker based setup for this
workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_apache_spark"&gt;1. Apache Spark&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Spark is a general purpose distributed analytic engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Spark can execute tasks in DAG with lazy evaluation for better query
optimization.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_spark_cluster_modes"&gt;2. Spark cluster modes&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Spark can be deployed in cluster form to execute tasks across multiple machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For this workshop we are covering the &lt;strong&gt;Spark over Yarn&lt;/strong&gt; setup. Which
is more scalable and has more cluster management options provided
by Yarn resource manager.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_standalone_cluster"&gt;Standalone cluster&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Run spark cluster without hadoop or any of the bellow mentioned
cluster managers. Spark shipped with this Standalone default
cluster manager. We can easily setup a spark cluster using this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Here we aren&amp;#8217;t covering it, Yarn based cluster manager has more
advanced features and resource controllers. We are focusing mainly
Yarn based cluster manager here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_spark_on_yarn"&gt;Spark on Yarn&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This document discuss more about how we handle the spark on Yarn
clster. Please read futher to see more on it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_spark_on_kubernetes"&gt;Spark on Kubernetes&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We will cover this on 4&amp;#8217;th session of Bigdata workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect4"&gt;
&lt;h5 id="_spark_on_mesos"&gt;Spark on Mesos&lt;/h5&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Mesos is another cluster resource manager like Yarn, we aren&amp;#8217;t
covering more on it here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_setup_spark_on_yarn"&gt;3. Setup Spark on Yarn&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_setup_hadoop_cluster"&gt;3.1. Setup Hadoop cluster&lt;/h3&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Mac and Windows users has to provide enough memory and cpus to the docker
daemon. On these machines the docker daemon runs under a virtual machine; the installer
setup all these. The default size gives is around 2 cpu core and 2gb. Ensure you provision
at least 5GB RAM and 3Core CPU for docker daemon to play around with bellow experiments.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We already covered this part, in our session 1. Lets add some extra
memory constraints when launching the nodes to get more idea on how
yarn manages the resources. Here we are exposing the required ports
to host machine to see the Web interfaces of RN and HDFS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Launch namenode with 2 core cpu, 2gb memory.
#
docker run -it -d --name namenode \
    -p 8088:8088 \
    -p 50070:50070 \
    --memory 2g --cpus 2
    --network hadoop-nw haridasn/hadoop-2.8.5 namenode
#
# 8088 - RN web interface
# 50070 - HDFS web interface.
#

#
# Datanode 1, 2

docker run -it -d --name datanode1 \
    --memory 2g --cpus 2 \
    --network hadoop-nw haridasn/hadoop-2.8.5 datanode &amp;lt;namenode-ip&amp;gt;

docker run -it -d --name datanode2 \
    --memory 2g --cpus 2 \
    --network hadoop-nw haridasn/hadoop-2.8.5 datanode &amp;lt;namenode-ip&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Check the container memory allocation status using &lt;code&gt;docker stats&lt;/code&gt; command.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_a_client_docker_container"&gt;3.2. Create a client docker container&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker run -it -d --name hadoop-cli --network hadoop-nw haridasn/hadoop-cli

# Add the hadoop configuration to hadoop-client container.
# You can skip this if you already done this.
docker cp namenode:/opt/hadoop/etc .
docker cp etc hadoop-cli:/opt/hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_run_spark_container"&gt;3.3. Run Spark Container&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;docker pull haridasn/spark-2.4.0

docker run -it -d --name spark --network hadoop-nw haridasn/spark-2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_connect_spark_with_yarn"&gt;3.4. Connect spark with Yarn&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Get the hadoop configurations.
docker cp namenode:/opt/hadoop/etc .

docker cp etc/hadoop spark:/opt/spark/hadoop-conf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_spark_user_dir_in_hdfs"&gt;3.5. Create spark user dir in HDFS&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;When connecting spark with YARN, you need a staging directory to manage the application
details on the HDFS. The variable which manages this is &lt;code&gt;spark.yarn.stagingDir&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Create a directory under hdfs using the &lt;code&gt;hadoop-cli&lt;/code&gt; hadoop client and ensure it got
correct directory permissions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;docker exec -it hadoop-cli bash
./bin/hdfs dfs -mkdir /user/spark
./bin/hdfs dfs -chown spark:spark /user/spark
./bin/hdfs dfs -ls /user/spark&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_lets_play_on_spark"&gt;3.6. Lets play on spark&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;# Connect to the spark container to play with it.
docker exec -it spark bash

# Inside the spark docker
#
export HADOOP_CONF_DIR=/opt/hadoop-conf

# Try on scala client.
spark-shell --master yarn --deploy-mode client

# Try on python client.
pyspark --master yarn --deploy-mode client


# connect via jupyter notebok, so we can use python to write
# spark jobs via pyspark.
jupyter notebook --no-browser --ip=0.0.0.0 --port 8090&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Checkout the command line options &lt;code&gt;pyspark --help&lt;/code&gt; to know more options
that we can try when submitting the jobs or running as client mode.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_view_the_full_cluster_health"&gt;3.7. View the full cluster health&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As we are running all the services via docker ensure that the containers are getting
enough resources so that we can play with spark using some smaller size data set to
under stand how the APIs and other features works in spark.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# To get the ideas about container resource consumption CPU/RAM/IO
# Ensure you have enough left.
docker stats&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Test setup is worked well on:-&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-text" data-lang="text"&gt;Test cluster setup on my laptop with 4 core CPU and 8GB memory.

Allocated

    5GB for docker daemon running on your laptop.
    3 Core for docker daemon on your laptop.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;My Setup:-&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://raw.githubusercontent.com/haridas/hadoop-env/master/tutorials/images/hadoop-spark-cluster.png" alt="Hadoop cluster image"&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_submit_jobs_into_spark_cluster"&gt;3.8. Submit jobs into spark cluster&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Try more examples from this link: &lt;a href="https://spark.apache.org/examples.html" class="bare"&gt;https://spark.apache.org/examples.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_presentation"&gt;3.9. Presentation&lt;/h3&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/iup0BsV0cNWjN0" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/haridasnss/apache-spark-on-hadoop-yarn-resource-manager" title="Apache spark on Hadoop Yarn Resource Manager" target="_blank"&gt;Apache spark on Hadoop Yarn Resource Manager&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="https://www.slideshare.net/haridasnss" target="_blank"&gt;haridasnss&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="hadoop"></category><category term="spark"></category><category term="yarn"></category></entry><entry><title>Bigdata Session 3 - Apache Drill</title><link href="http://localhost:8000/bigdata-session-3-apache-drill.html" rel="alternate"></link><published>2019-06-02T00:00:00+05:30</published><updated>2019-06-02T00:00:00+05:30</updated><author><name>HN</name></author><id>tag:localhost,2019-06-02:/bigdata-session-3-apache-drill.html</id><summary type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This is a series of blog posts about the 4 session I conducted at different engieneering
colleges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The workshop material also …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This is a series of blog posts about the 4 session I conducted at different engieneering
colleges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The workshop material also included in a slide, please refer bellow slide for
details about apache drill and how to play with it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_presentation"&gt;1. Presentation&lt;/h3&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/3AUAyy8iZRqvM3" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/haridasnss/apache-drill-scalable-sql-query-engine" title="Apache drill - Scalable SQL query Engine" target="_blank"&gt;Apache drill - Scalable SQL query Engine&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="https://www.slideshare.net/haridasnss" target="_blank"&gt;haridasnss&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_references"&gt;2. References&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.rittmanmead.com/blog/2017/04/sql-on-hadoop-impala-vs-drill/" class="bare"&gt;https://www.rittmanmead.com/blog/2017/04/sql-on-hadoop-impala-vs-drill/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="drill"></category><category term="yarn"></category></entry><entry><title>Bigdata Session 4 - Apache Spark on Kubernetes</title><link href="http://localhost:8000/bigdata-session-4-apache-spark-on-kubernetes.html" rel="alternate"></link><published>2019-06-02T00:00:00+05:30</published><updated>2019-06-02T00:00:00+05:30</updated><author><name>HN</name></author><id>tag:localhost,2019-06-02:/bigdata-session-4-apache-spark-on-kubernetes.html</id><summary type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This is a series of blog posts about the 4 session I conducted at different engieneering
colleges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_spark_on_kubernetes"&gt;1. Spark on Kubernetes …&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="paragraph"&gt;
&lt;p&gt;This is a series of blog posts about the 4 session I conducted at different engieneering
colleges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 1 - Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 2 - Apache Spark on Apache Hadoop&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 3 - Apache Drill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bigdata Session 4 - Apache Spark on Kubernetes &amp;#8592; ( You are here now )&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_spark_on_kubernetes"&gt;1. Spark on Kubernetes&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Kubernetes is a linux container manager, the ideas is similar to how yarn manages the
jvm containers in hadoop environment. Kubernetes can be used to deploy very hetrogenious
workloads, and it can meet a requirements of an entire business. eg; Deploy applications
, dev/stage environments, offline or batch processing services etc. As the hadoop
environment is specific to bigdata processing, here we can use existing kubernetes
cluster to do the things that we done on hadoop cluster or spark cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This tutorial covers how we can quickly setup a kubernetes cluster and deploy a
spark cluster on it, so that then we can play on spark. The kubernetes act as one
of the spark&amp;#8217;s cluster manager, there is no change in other aspects of how spark
does it&amp;#8217;s functionalities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_setup_spark_on_kubernetes"&gt;2. Setup spark on kubernetes&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Kubernet can be deploy in multi-node or single node environment similar to
hadoop cluster. Here we try the kubernetes setup on a VM.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We will be using &lt;code&gt;minikube&lt;/code&gt; tool to setup kubernetes cluster. Minikube provide an easy
way to setup kubernetes cluster on a VM for testing and experiment purpose.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_gnulinux_environment"&gt;2.1. Gnu/Linux Environment:-&lt;/h3&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install virtualbox&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow this link to install minikube &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/#install-minikube" class="bare"&gt;https://kubernetes.io/docs/tasks/tools/install-minikube/#install-minikube&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install kubectl - &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-native-package-management" class="bare"&gt;https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-native-package-management&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Start the minikube to launch the kubernetes cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;minikube start --cpus 3 --memory 6000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_mac_environment"&gt;2.2. Mac environment:-&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;brew cask install minikube
brew install docker-machine-driver-hyperkit
minikube start --vm-driver hyperkit --cpus 3 --memory 6000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; command available in mac if you already have the docker installed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_test_kubernetes_cluster_is_up"&gt;2.3. Test kubernetes cluster is up&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;kubectl cluster-info&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Copy the master URL, we need it bellow to submit spark jobs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;kubectl create namespace spark1
kubectl create serviceaccount jumppod -n spark1
kubectl create rolebinding jumppod-rb --clusterrole=admin --serviceaccount=spark1:jumppod -n spark1
kubectl run jump-1 -ti --rm=true -n spark1 --image=brainlounge/jumppod:ubuntu-18.04 --serviceaccount='jumppod'&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_ensure_you_have_jdk_1_8_installed_in_your_laptop"&gt;2.4. Ensure you have JDK 1.8 installed in your laptop&lt;/h3&gt;

&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_prepare_spark_container_images_for_kubernetes_cluster"&gt;2.5. Prepare spark container images for kubernetes cluster&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Kubernetes is a container manager or orchestrator, we need to package the spark in
docker image form to deploy them on kubernetes cluster.
&lt;code&gt;spark-submit&lt;/code&gt; command support the spark job submission into kubernetes cluster
by just changing the &lt;code&gt;--master url&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;div class="title"&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Run this after the kubernetes cluster is up.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Download a copy of the spark binary into your laptop and build the docker
# image from it.
wget http://mirrors.estointernet.in/apache/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
cd spark-2.4.0-bin-hadoop2.7
./bin/docker-image-tool.sh -m -t 2.4.0 build&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_start_a_spark_client_on_kubernetes_cluster_manager"&gt;2.6. Start a spark client on kubernetes cluster manager&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Get the kubernetes master url, will be in this form 'https://&amp;lt;host:port&amp;gt;'.
minikube cluster-info

cd spark-2.4.0-bin-hadoop2.7
./bin/spark-shell --master k8s://https://&amp;lt;host:port&amp;gt; --name spark-kube-cli --deploy-mode client \
    --conf spark.kubernetes.container.image=spark:2.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we can try with all the spark command features available, only change here
is the &lt;code&gt;--master&lt;/code&gt; param and extra &lt;code&gt;--conf&lt;/code&gt; with image name for the spark.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_why_spark_on_kubernetes"&gt;3. Why spark on kubernetes&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes can work with wide variety of application cluster, your entire application
stack it can host.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Easy to deploy and manager different types of applications and its different stages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have a kubernetes cluster in your infrastructure, this is the best option
available to run a spark.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_presentation"&gt;4. Presentation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/sRm6qq2Fdaplhk" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt; &lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="//www.slideshare.net/haridasnss/apache-spark-on-kubernetes" title="Apache Spark on Kubernetes" target="_blank"&gt;Apache Spark on Kubernetes&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="https://www.slideshare.net/haridasnss" target="_blank"&gt;haridasnss&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="hadoop"></category><category term="spark"></category><category term="drill"></category><category term="kubernetes"></category></entry></feed>