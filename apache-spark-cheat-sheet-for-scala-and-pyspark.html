<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="HN, ">

        <link rel="alternate"  href="https://haridas.in/feeds/all.atom.xml" type="application/atom+xml" title="HN Full Atom Feed"/>

    <title>Apache Spark cheat sheet for scala and pyspark // all posts // HN </title>


    <link href='https://fonts.googleapis.com/css?family=PT+Mono|PT+Serif|PT+Sans' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://haridas.in/theme/css/pure.css">
    <link rel="stylesheet" href="https://haridas.in/theme/css/asciidoctor.css">

    <link rel="stylesheet" href="https://haridas.in/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.2.0/jquery.fitvids.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="container">

            <div class="container-header">
            </div>


            <div class="container-body">

                    <div class="body-sidebar">
                        <header class="content-sidebar">

                            <hgroup>

                                <img class="avatar" src="https://haridas.in/images/profile_pic.jpg">

                                <h1 class="brand-main"><a href="https://haridas.in">HN</a></h1>


                                            <p class="links">
                                                <a href="https://haridas.in/category/programming.html">Programming
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/category/data-science.html">Data-Science
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/tag/security.html">Security
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/category/devops.html">Devops
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/pages/my-talks.html">Talks
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/pages/about-me.html">About Me
                                                </a>
                                            </p>
                                            <p class="links">
                                                <a href="https://haridas.in/resume.pdf">Resume
                                                </a>
                                            </p>
                                <p class="social">
                                        <a href="https://twitter.com/haridas_n">
                                                <img src="https://haridas.in/images/tt.svg" alt="menu icon" width="48" height="48" />
                                        </a>
                                        <a href="https://github.com/haridas">
                                                <img src="https://haridas.in/images/github.svg" alt="menu icon" width="45" height="45" />
                                        </a>
                                        <a href="https://linkedin.com/in/haridasn">
                                                <img src="https://haridas.in/images/linkedin.svg" alt="menu icon" width="48" height="48" />
                                        </a>
                                </p>
                            </hgroup>
                    </header>

                    </div>


                    <div class="body-column">
    <div class="content">
        <div class="column">
            <section class="post">
                <header class="post-header">
                    <h1 class="article-title">Apache Spark cheat sheet for scala and pyspark</h1>
                        <p class="post-meta">
                            Tags:                                 <a class="post-category" href="https://haridas.in/tag/spark.html">spark</a>
                                <a class="post-category" href="https://haridas.in/tag/dataframe.html">dataframe</a>
                                <a class="post-category" href="https://haridas.in/tag/pyspark.html">pyspark</a>
                        </p>


<a href="https://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="haridas_n">
    Tweet
</a>

<script type="text/javascript" src="https://platform.twitter.com/widgets.js"></script>

<!-- Place this tag where you want the +1 button to render. -->
<div class="g-plusone" data-size="medium"></div>

<!-- Place this tag after the last +1 button tag. -->
<script type="text/javascript">
  (function() {
       var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
           po.src = 'https://apis.google.com/js/platform.js';
               var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
                 })();
</script>                        <p class="post-date">
                            in <a href="https://haridas.in/category/data-science.html">data-science</a> &middot; Mon 15 April 2019
                        </p>
                </header>
            </section>
            <div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel2">
<li><a href="#_read_the_partitioned_json_files_from_disk">Read the partitioned json files from disk</a></li>
<li><a href="#_save_partitioned_files_into_a_single_file">Save partitioned files into a single file.</a></li>
<li><a href="#_filter_rows_which_meet_particular_criteria">Filter rows which meet particular criteria</a></li>
<li><a href="#_map_with_case_class">Map with case class</a></li>
<li><a href="#_use_selectexpr_to_access_inner_attributes">Use selectExpr to access inner attributes</a></li>
<li><a href="#_how_to_access_rdd_methods_from_pyspark_side">How to access RDD methods from pyspark side</a></li>
<li><a href="#_filtering_a_dataframe_column_of_type_seqstring">Filtering a DataFrame column of type Seq[String]</a></li>
<li><a href="#_filter_a_column_with_custom_regex_and_udf">Filter a column with custom regex and udf</a></li>
<li><a href="#_sum_a_column_elements">Sum a column elements</a></li>
<li><a href="#_remove_unicode_characters_from_tokens">Remove Unicode characters from tokens</a></li>
<li><a href="#_connecting_to_jdbc_with_partition_by_integer_column">Connecting to jdbc with partition by integer column</a></li>
<li><a href="#_parse_nested_json_data">Parse nested json data</a></li>
<li><a href="#_string_arraystring_conversion">"string &#8658; array&lt;string&gt;" conversion</a></li>
<li><a href="#_a_crazy_string_collection_and_groupby">A crazy string collection and groupby</a></li>
<li><a href="#_how_to_access_aws_s3_on_spark_shell_or_pyspark">How to access AWS s3 on spark-shell or pyspark</a></li>
<li><a href="#_set_spark_scratch_space_or_tmp_directory_correctly">Set spark scratch space or tmp directory correctly</a></li>
<li><a href="#_pyspark_doesnt_support_all_the_data_types">Pyspark doesn&#8217;t support all the data types.</a></li>
<li><a href="#_work_with_spark_standalone_cluster_manager">Work with spark standalone cluster manager</a></li>
<li><a href="#_changelog">Changelog</a></li>
<li><a href="#_references">References</a></li>
</ul>
</div>
<div class="paragraph">
<p>This page contains a bunch of spark pipeline transformation methods, which
we can use for different problems. Use this as a quick cheat on how we can
do particular operation on spark dataframe or pyspark.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This code snippets are tested on <code>spark-2.4.x</code> version, mostly work on
<code>spark-2.3.x</code> also, but not sure about older versions.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_read_the_partitioned_json_files_from_disk">Read the partitioned json files from disk</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-spark" data-lang="spark">val vocabDist = spark.read
    .format("json")
    .option("mergeSchema", "true")
    .load("/mnt/all_models/run-26-nov-2018-clean-vocab-50k-4m/model/topic-description"</code></pre>
</div>
</div>
<div class="paragraph">
<p>applicable to all types of files supported</p>
</div>
</div>
<div class="sect2">
<h3 id="_save_partitioned_files_into_a_single_file">Save partitioned files into a single file.</h3>
<div class="paragraph">
<p>Here we are merging all the partitions into one file and dumping it into
the disk, this happens at the driver node, so be careful with sie of
data set that you are dealing with. Otherwise, the driver node may go out of memory.</p>
</div>
<div class="paragraph">
<p>Use <code>coalesce</code> method to adjust the partition size of RDD based on our needs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">spark.coaleace(1)
    .write
    .json("/mnt/test.json")</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_filter_rows_which_meet_particular_criteria">Filter rows which meet particular criteria</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">vocabDist
    .filter($"topic" === 0)
    .select("term")
    .filter(x =&gt; x.toString.stripMargin.length == 3)
    .count()

// Find minimal value of data frame.
vocabDist
    .filter("topic == 0")
    .select("term")
    .map(x =&gt; x.toString.length)
    .agg(min("value"))
    .show()</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_map_with_case_class">Map with case class</h3>
<div class="paragraph">
<p>Use case class if you want to map on multiple columns with a complex
data structure.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">case class Person(name: String, address: Array[String])

// Use the case-class to get the more controll on the types when
// Filtering the dataFrames.
val personDf
	.select("name", "address")
	.as[Person]
	.map(p =&gt; (p.name, p.address[0]))
	.toDF("name", "primary_address")</code></pre>
</div>
</div>
<div class="paragraph">
<p>OR using <code>Row</code> class.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>import org.apache.spark.sql.Row
val newDf = personDf.map {
	case Row(name: String, address: Array[String]) =&gt; (name, address[0])
}.toDF("name", "primary_address")</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_selectexpr_to_access_inner_attributes">Use selectExpr to access inner attributes</h3>
<div class="paragraph">
<p>Provide easily access the nested data structures like <code>json</code> and filter them
using any existing udfs, or use your udf to get more flexibility here.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">// Select only those documents which has some features in it.
doc.selectExpr("doc_id", "size(features.values) as feature_size")
 .where("feature_size &gt; 0")</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_access_rdd_methods_from_pyspark_side">How to access RDD methods from pyspark side</h3>
<div class="paragraph">
<p>Using standard <code>RDD</code> operation via pyspark API isn&#8217;t straight forward, to get that
we need to invoke the <code>.rdd</code> to convert the DataFrame to support these features.</p>
</div>
<div class="paragraph">
<p>For example, here we are converting a sparse vector to dense and summing it in column-wise.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">vocab_freq = countVector\
    .select("features")\
    .rdd\
    .map(lambda x: SparseVector(x[0].size, x[0].indices, x[0].values).toArray())\
    .reduce(lambda a, b: a + b)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Pyspark Map on multiple columns</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python"># Find document length.
doc_lengths = countVector\
    .selectExpr("doc_id", "features.indices as indicies", "features.values as feature_freq")\
    .select("doc_id", "feature_freq")\
    .rdd.map(lambda x: Row(doc_id=x[0], doc_length=sum(x[1])))\
    .toDF()</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_filtering_a_dataframe_column_of_type_seqstring">Filtering a DataFrame column of type Seq[String]</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">val dataPqFiltered = dataPq
    .selectExpr("length(abstract_html_strip) as doc_len", "*")
    .filter("doc_len &gt; 3")</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_filter_a_column_with_custom_regex_and_udf">Filter a column with custom regex and udf</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">val tokenFilter = udf((arr: Seq[String]) =&gt; arr.filter(_.matches("\\w+-\\w+|\\w+-|\\w+")))
val tokenCounter = udf((arr: Seq[String]) =&gt; arr.length))


val tokenSamples = dataPqCleaned
    .withColumn("tokenFiltered",tokenCounter(tokenFilter(dataPqCleaned("abstract_tokenized"))))</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sum_a_column_elements">Sum a column elements</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">DataFrame.select(sum($"tokenFiltered")).show()
Other function examples are "avg", "std" etc.. Refer org.apache.spark.sql.functions._</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_remove_unicode_characters_from_tokens">Remove Unicode characters from tokens</h3>
<div class="paragraph">
<p>Sometimes we only need to work with the ascii text, so it&#8217;s better to clean out
other chars.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">val tokenFilterFlat = udf((arr: Seq[String]) =&gt; arr.flatMap(
    "\\w+-\\w+|\\w+-|\\w+".r.findAllIn(_)).filter(_.length &gt; 3))

val tokenFilter = udf((arr: Seq[String]) =&gt; arr.filter(_.matches("\\w+-\\w+|\\w+-|\\w+")))
val tokenCounter = udf((arr: Seq[String]) =&gt; arr.length)
val minLengthFilter = udf((arr: Seq[String]) =&gt; arr.filter(_.length &gt; 3))</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_connecting_to_jdbc_with_partition_by_integer_column">Connecting to jdbc with partition by integer column</h3>
<div class="paragraph">
<p>When using the spark to read data from the SQL database and then do the
other pipeline processing on it, it&#8217;s recommended to partition the data
according to the natural segments in the data, or at least on an integer
column, so that spark can fire multiple sql queries to read data from SQL
server and operate on it separately, the results are going to the spark
partition.</p>
</div>
<div class="paragraph">
<p>Bellow commands are in pyspark, but the APIs are the same for the scala version also.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">jdbc_url = "jdbc://..."
src_conn_prop =  {
    //
}

data_query = "(select * from reporting limit 100000)data"
report_ids = spark.read.jdbc(url = jdbc_url,
                        table = data_query,
                        lowerBound = 1,
                        column = "report_id",
                        upperBound = 603442,
                        numPartitions = 3,
                        properties = src_conn_prop)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parse_nested_json_data">Parse nested json data</h3>
<div class="paragraph">
<p>This will be very helpful when working with <code>pyspark</code> and want to pass very
nested json data between JVM and Python processes. Lately spark community relay on
apache arrow project to avoid multiple serialization/deserialization costs when
sending data from java memory to python memory or vice versa.</p>
</div>
<div class="paragraph">
<p>So to process the inner objects you can make use of this <code>getItem</code> method
to filter out required parts of the object and pass it over to python memory via
arrow. In the future arrow might support arbitrarily nested data, but right now it won&#8217;t
support complex nested formats. The general recommended option is to go without nesting.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-spark" data-lang="spark">doc_features
   .select($"features".getItem("values").alias("vocab_count"))
   .select(size($"vocab_count").alias("unique_features"))
   .groupBy("unique_features")
   .count()
   .show()</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_string_arraystring_conversion">"string &#8658; array&lt;string&gt;" conversion</h3>
<div class="paragraph">
<p>Type annotation <code>.as[String]</code> avoid implicit conversion assumed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">    df.select("column").as[String].map(x =&gt; Seq(x.toString))</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_a_crazy_string_collection_and_groupby">A crazy string collection and groupby</h3>
<div class="paragraph">
<p>This is a stream of operation on a column of type <code>Array[String]</code> and collect
the tokens and count the n-gram distribution over all the tokens.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-scala" data-lang="scala">dataset_sample
    .select("chunks").as[Array[String]]
    .collect
    .flatten
    .distinct
    .map(x =&gt; x.split(" ").length)
    .zipWithIndex
    .groupBy(_._1)
    .map { case (k, v) =&gt; (k, v.size) }
    .toArray
    .sortBy(_._1)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_how_to_access_aws_s3_on_spark_shell_or_pyspark">How to access AWS s3 on spark-shell or pyspark</h3>
<div class="paragraph">
<p>Most of the time we might require a cloud storage provider like s3 / gs etc, to
read and write the data for processing, very few keeps in-house hdfs to handle the data
themself, but for majority, I think cloud storage easy to start with and don&#8217;t need
to bother about the size limitations.</p>
</div>
<div class="paragraph">
<p>Here is the quick snippet to connect with s3.</p>
</div>
<div class="sect3">
<h4 id="_supply_the_aws_credentials_via_environment_variable">Supply the aws credentials via environment variable</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">// Export these two envs before running `spark-shell`.
export AWS_SECRET_KEY=
export AWS_ACCESS_KEY=

spark-shell --packages org.apache.hadoop:hadoop-aws:2.7.7 --master &lt;master-url&gt;

import com.amazonaws.auth._
val envReader = new EnvironmentVariableCredentialsProvider()
spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", envReader.getCredentials().getAWSAccessKeyId)
spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", envReader.getCredentials().getAWSSecretKey)
spark.sparkContext.hadoopConfiguration.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_supply_the_credentials_via_default_aws_awsconfig_file">Supply the credentials via default aws ~/.aws/config file</h4>
<div class="paragraph">
<p>Recent versions of <code>awscli</code> expect its configurations are kept under <code>~/.aws/credentials</code> file,
but old versions looks at <code>~/.aws/config</code> path, spark 2.4.x version now looks at the <code>~/.aws/config</code> location
since spark 2.4.x comes with default hadoop jars of version 2.7.x.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">// Configure the spark to read from s3. Ensure the
// aws config file is set at ~/.aws/config path.
import com.amazonaws.auth.profile.ProfilesConfigFile

val profileReader = new ProfilesConfigFile().getCredentials("default")
spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", profileReader.getAWSAccessKeyId)
spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", profileReader.getAWSSecretKey)
spark.sparkContext.hadoopConfiguration.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_set_spark_scratch_space_or_tmp_directory_correctly">Set spark scratch space or tmp directory correctly</h3>
<div class="paragraph">
<p>This might require when working with a huge dataset and your machine can&#8217;t hold them
all in memory for given pipeline steps, those cases the data will be spilled over
to disk, and saved in tmp directory.</p>
</div>
<div class="paragraph">
<p>Set bellow properties to ensure, you have enough space in tmp location.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">#vim ./conf/spark-defaults.conf

...
spark.local.dir   /mnt/spark-tmp
spark.executor.extraJavaOptions /mnt/spark-tmp
spark.driver.extraJavaOptions /mnt/spark-tmp

...</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pyspark_doesnt_support_all_the_data_types">Pyspark doesn&#8217;t support all the data types.</h3>
<div class="paragraph">
<p>When using the <code>arrow</code> to transport data between jvm to python memory, the arrow may throw
bellow error if the types aren&#8217;t compatible to existing converters. The fixes may become
in the future on the arrow&#8217;s project. I&#8217;m keeping this here to know that how the pyspark gets
data from jvm and what are those things can go wrong in that process.</p>
</div>
<div class="paragraph">
<p>Example 1:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">    arrs = [create_array(s, t) for s, t in series]
  File "/home/ubuntu/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py", line 251, in create_array
    return pa.Array.from_pandas(s, mask=mask, type=t)
  File "pyarrow/array.pxi", line 531, in pyarrow.lib.Array.from_pandas
  File "pyarrow/array.pxi", line 171, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 80, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 89, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: NumPyConverter doesn't implement &lt;list&lt;item: int32&gt;&gt; conversion.</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_work_with_spark_standalone_cluster_manager">Work with spark standalone cluster manager</h3>
<div class="paragraph">
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-standalone-example-2-workers-on-1-node-cluster.html" class="bare">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-standalone-example-2-workers-on-1-node-cluster.html</a></p>
</div>
<div class="sect3">
<h4 id="_start_the_spark_clustering_in_standalone_mode">Start the spark clustering in standalone mode</h4>
<div class="paragraph">
<p>Once you have downloaded the same version of the spark binary across the machines
you can start the spark master and slave processes to form the standalone spark
cluster. Or you could run both these services on the same machine also.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">cd spark-&lt;version&gt;

# Start the spark master process, provide the master configurations via
# properties file or add it in default config file under the conf folder.
./sbin/start-master.sh [--properties-file &lt;file&gt;]


# Start slave services on each node where we want to run the slave and connect
# All the slaves to master to form the cluster.
#
./sbin/start-slave.sh -c 2 -m 16g spark://master-host:7077</code></pre>
</div>
</div>
<div class="paragraph">
<p>Standalone mode,</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Worker can have multiple executors.</p>
</li>
<li>
<p>Worker is like a node manager in yarn.</p>
</li>
<li>
<p>We can set worker max core and memory usage settings.</p>
</li>
<li>
<p>When defining the spark application via spark-shell or so, define the executor memory and cores.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>eg; worker-1 has 10 core and 20gb memory</p>
</div>
<div class="paragraph">
<p>When submitting the job to get 10 executor with 1 cpu and 2gb ram each,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>spark-submit --execture-cores 1 --executor-memory 2g --master &lt;url&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This page will be updated as and when I see some reusable snippet of code for spark operations
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_changelog">Changelog</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Added spark standalone commands.</p>
</li>
</ol>
</div>
</div>
<div class="sect1">
<h2 id="_references">References</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><a href="https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html" class="bare">https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html</a></p>
</li>
</ol>
</div>
</div>
</div>
            <a href="#" class="go-top">Go Top</a>
    <div class="comments">

        <!-- <div id="disqus_thread"></div> -->

        <div id="disqus_thread">
            <a href="#" class="comments-holder" onclick="loadDisqus();return false;">Show/Post Comments</a> 
        </div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = "haridas"; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            var loadDisqus = function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            };
        </script>

        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

    </div>
        </div>
    </div>
                    </div>

            </div>

            <div class="container-footer">
<footer class="footer">
                <a href="https://twitter.com/haridas_n"> Twitter</a> |
                <a href="https://github.com/haridas"> Github</a> |
                <a href="https://linkedin.com/in/haridasn"> Linkedin</a>
        &ndash;
        &copy; HN &ndash;
        <i> <a href="https://github.com/haridas/hn-theme">Built with HN Theme</a> </i>
        for <a href="https://blog.getpelican.com/">Pelican</a>
</footer>            </div>

    </div>

    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
    <script type="text/javascript">

      var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-23592173-1']);
          _gaq.push(['_trackPageview']);

    (function() {
     var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
     ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
         var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
       })();

    </script>

</body>
</html>