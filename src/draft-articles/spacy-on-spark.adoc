:title: Run spacy jobs on spark
:date: 13-Apr-2019
:category: data-science
:tags: bigdata,nlp,spark
:toc:
:numbered:

= Run spacy jobs on spark using pyspark

Spacy is a state-of-the-art NLP library in Python, which provides a lot of 
tools that required in NLP problems, eg; NER, unicode tokenizers, Deep learning
models for Tagging, NER and related operations.

Spark provides only traditional NLP tools like standard tokenizers, tf-idf,etc,
we mostly need accurate POS tagging and chunking features when working with
NLP problems, which spark libraries aren't close to spacy. Those cases we need to
relay on spacy.

=== Pyspark architecture

A quick brief about the pyspark architecture, Bellow image shows that the workers
spawn python process to run the pyspark jobs; which are written in python and all
necessary python ML libraries like sklearn, numpy, spacy etc.

image:[http://i.imgur.com/YlI8AqEl.png](Pyspark Architecture)

The spark worker doesn't control much other than starting the python worker
process and control whether python worker need to be restarted on every job or
not.

=== Main challenges when using spacy models with pyspark

1. How to skip spacy model serialization
2. Spacy's inbuilt multi-processing feature may bite you.
3. How we can manage the worker process management.


Usually when we submit spark jobs to the spark `driver` compiles it and optimize the
pipeline. The final plan of the pipeline is split across the `executors` based on
the DAG of data flow defined on the pipeline. Here the spark executors does the 
actual work, where the driver program sends out the relevant codes to executes 
at executor side. This is being done by serializing the relevant parts of the
pipeline.

One thing to ensure is our program is serializable ( Source code, classes and objects ).
Otherwise spark fails to execute the pipeline.

=== How to skip spacy model serialization

How we ensure this is by avoiding the scenario of serializing the spacy's inbuilt
trained binary models. How we do that ? 

Pyspark uses `PickleSerializer` to serialize the python objects, but spacy models
aren't serialisable using `PickleSerializer` which is trigger the issue when we
load the spacy model first and then refer in worker code.

==== This code will fail 
```python
// THIS FAILS
import spacy
form pyspark.sql.functions import pandas_udf, PandasUDFTypes
from pyspark.sql.type import StringType, ArrayType


SPACY_MODEL = spacy.load("en_core_web_lg") 
    
#
# Simple UDF function which uses the spacy model to evaluate your create 
#
@pandas_udf(returnValue=ArrayType(StringType()), functionType=PandasUDFType.SCALAR):
def tokenize_and_clean(documents):
    docs = SPACY_MODEL.pipe(documents)

data_df = spark.read.parquet("/data/documents.parquet")

df = df.select("doc_id", "document").\
    withColumn("tokens", tokenize_and_clean("document")).\
    .select("doc_id", "tokens")
```

==== Working Version

Here we are wrapping the spacy model under a lazy function, which will ensure
the model won't get loaded until it's really required -- which is actually required
when the executor runs this code with partitioned dataset.

```python
import spacy
form pyspark.sql.functions import pandas_udf, PandasUDFTypes
from pyspark.sql.type import StringType, ArrayType


SPACY_MODEL = None
def get_spacy_model():
    global SPACY_MODEL
    if not SPACY_MODEL:
       _model = spacy.load("en_core_web_lg") 
    SPACY_MODEL = _model
    return SPACY_MODEL
    
#
# Simple UDF function which uses the spacy model to evaluate your create 
#
@pandas_udf(returnValue=ArrayType(StringType()), functionType=PandasUDFType.SCALAR):
def tokenize_and_clean(documents):
    spacy_model = get_spacy_model()
    docs = spacy_model.pipe(documents)


data_df = spark.read.parquet("your_file.parquet")

df.select("doc_id", "document").
    withColumn("tokens", tokenize_and_clean("document"))
```

At driver side we won't load the spacy model, instead ensure they are loaded lazily at
executor side.

Here the models can't be serialised at the driver side and ship it to worker and
load it back, So we need to ensure only at the runtime the models are really gets
loaded into the worker memory.


=== Spacy multi-processing capabilities

This feature included with spacy to speed up the pipeline processing
and making use of multiple core available on the machine.

But this feature not be a good candidate for execute with spark environment.


=== Worker process management

Spark cluster configuration for spacy


1. worker - 1 with 1 cpu core, and 5 gb or so ram.
2. executor - 1, uses full resources of worker.
3. Spacy pipeline with multi-processing support.

Even if we don't enable `nlp.pipe` api, after first document processing, 
the nlp object goes into multip processing mode, it may be a bug on the spacy 
side; I couldn't find any option to disable the multi processing option on spacy.
Because of this we have to take care at the spark side to avoid running multiple
executor per node and ensure the executor won't use more than one cpu core to
avoid spark side optimization by spwaning more threads if cores are available
for executor.

== Run spark with below configuration

=== master

```bash
./sbin/start-master
```

=== slave 1
```bash
./sbin/start-slave.sh -c 1 -m 5g spark://<master-hostname>:7077
```

IMPORTANT: Here we are setting spark worker to use only one CPU, this
means spark can launch one executor with 1 CPU, as with spacy workload
main computation happening at python side, and spacy brings the multiprocessing 
outside the spark framework.

=== Check the python processes ran by each spark worker

```bash

On a 8 core machine, if we configured it in 

Standalone cluster manager settings, 

ubuntu@ip-10-50-168-159:/mnt/input/coherence_eval$ pstree -asp | grep pyspark
  |   |   |-python,29960 -m pyspark.daemon
  |   |   |   `-python,29987 -m pyspark.daemon
  |   |   |-python,29973 -m pyspark.daemon
  |   |   |   `-python,30018 -m pyspark.daemon
  |   |   |-python,29975 -m pyspark.daemon
  |   |   |   `-python,30032 -m pyspark.daemon
  |   |   |-python,29978 -m pyspark.daemon
  |   |   |   `-python,30042 -m pyspark.daemon
  |   |   |-python,28303 -m pyspark.daemon
  |   |   |   `-python,28801 -m pyspark.daemon
  |   |   |-python,28651 -m pyspark.daemon
  |   |   |   `-python,29146 -m pyspark.daemon
  |   |   |-python,29962 -m pyspark.daemon
  |   |   |   `-python,30017 -m pyspark.daemon
  |   |   |-python,29972 -m pyspark.daemon
  |   |   |   `-python,30036 -m pyspark.daemon
          |-grep,30082 --color=auto pyspark

```

For  `Yarn` or `Kubernetes` cluster manager this problem won't happen as both
will restrict the system view to application restricted similar to VMs with the
help of Control Group (`cgroup`) and `namespace` feature. So the spark executor
or the python worker won't see entire CPU / RAM for utilization, they gets it by
the allocation specified based on the container spec on both Yarn and Kubernetes 
environment.



=== Takeaway

1. Ensure you are writing spark pipeline with serialisable objects, or do lazy
   evaluation.
2. Be careful when using the external libraries like spacy, which may bring its own
   multiprocessing feature, which will result in overloading the system with spark
   executor configuration.
3. Use the different cluster manager other than standalone one to get more control 
   over allocating resources to the executors.

=== References

1. Holden's blog - https://blog.dominodatalab.com/making-pyspark-work-spacy-overcoming-serialization-errors/
2. 
