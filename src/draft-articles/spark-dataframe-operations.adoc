:title: Apache Spark DataFrame filtering
:date: 12-08-2018
:category: datascience

=== Read the partitioned json files from disk

```spark
val vocabDist = spark.read
    .format("json")
    .option("mergeSchema", "true")
    .load("/mnt/all_models/run-26-nov-2018-clean-vocab-50k-4m/model/topic-description"
```
applicable to all types of files supported 

=== Save partitioned files into a single file.

Here we are merging all the partitions into one file and dumping it into 
the disk, most cases this happens at the driver node, so be care ful the sie of
data set that you are dealing with. Otherwise the driver node may go out of memory.

spark.coaleace(1).write

```scala
vocabDist
    .filter($"topic" === 0)
    .select("term")
    .filter(x => x.toString.stripMargin.length == 3)
    .count()

// Find minimal value of data frame.
vocabDist
    .filter($"topic" === 0)
    .select("term")
    .map(x => x.toString.length)
    .agg(min("value"))
    .show()

```
== Mapping and Map Partition

== Filtering
=== Filtering a DataFrame column of type Seq[String]

```scala
val dataPqFiltered = dataPq
    .selectExpr("length(abstract_html_strip) as doc_len", "*")
    .filter("doc_len > 3")

Filter a column with custom regex.

val tokenFilter = udf((arr: Seq[String]) => arr.filter(_.matches("\\w+-\\w+|\\w+-|\\w+")))
val tokenCounter = udf((arr: Seq[String]) => arr.length))


val tokenSamples = dataPqCleaned
    .withColumn("tokenFiltered",tokenCounter(tokenFilter(dataPqCleaned("abstract_tokenized"))))
```
==== Sum a column elements
DataFrame.select(sum($"tokenFiltered")).show()
Other function exmples are "avg", "std" etc.. Refer org.apache.spark.sql.functions._


=== Remove unicode characters from tokens

```scala
val tokenFilterFlat = udf((arr: Seq[String]) => arr.flatMap(
    "\\w+-\\w+|\\w+-|\\w+".r.findAllIn(_)).filter(_.length > 3))

val tokenFilter = udf((arr: Seq[String]) => arr.filter(_.matches("\\w+-\\w+|\\w+-|\\w+")))
val tokenCounter = udf((arr: Seq[String]) => arr.length)
val minLengthFilter = udf((arr: Seq[String]) => arr.filter(_.length > 3))

```

=== Connecting to jdbc with partition by integer column

```scala
data_query = "(select * from analyst_etl_app.reporting limit 100000)data"
pat_ids=spark.read.jdbc(url = src_jdbcUrl,
                        table = data_query,
                        lowerBound = 1,
                        column = 'pat_id',
                        upperBound = 603442,
                        numPartitions = 3,
                        properties = src_conn_prop)
                        
```

=== Parse nested json data

```spark
doc_features
   .select($"features".getItem("values").alias("vocab_count"))
   .select(size($"vocab_count").alias("unique_features"))
   .groupBy("unique_features")
   .count()
   .show()
```

== string => array<string> conversion

```scala
df.select("column").as[String].map(x => Seq(x.toString))
```

== A crazy string collection and groupby

```scala
dataset_sample
    .select("chunks").as[Array[String]]
    .collect
    .flatten
    .distinct
    .map(x => x.split(" ").length)
    .zipWithIndex
    .groupBy(_._1)
    .map { case (k, v) => (k, v.size) }
    .toArray
    .sortBy(_._1)
```

== Pyspark doesn't support all the datatpes.

Example 1:

```text
    arrs = [create_array(s, t) for s, t in series]
  File "/home/ubuntu/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py", line 251, in create_array
    return pa.Array.from_pandas(s, mask=mask, type=t)
  File "pyarrow/array.pxi", line 531, in pyarrow.lib.Array.from_pandas
  File "pyarrow/array.pxi", line 171, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 80, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 89, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: NumPyConverter doesn't implement <list<item: int32>> conversion. 
```
== References

1. https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html
